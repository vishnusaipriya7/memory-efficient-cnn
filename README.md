# ğŸ§  Memory-Efficient CNN for Low-Resource Devices

## ğŸ“Œ Overview
This project implements a **lightweight Convolutional Neural Network (CNN)** optimized for **low-resource and edge devices**. The model is designed using **depthwise separable convolutions** to reduce computation and memory usage, and is further optimized using **8-bit (INT8) post-training quantization** with **TensorFlow Lite**.

The optimized model is benchmarked for **CPU inference latency** and deployed using a **Python-based TensorFlow Lite inference application**, simulating real-world edge deployment.

---

## ğŸ¯ Key Features
- Lightweight CNN architecture using **depthwise separable convolutions**
- **Full INT8 quantization** using TensorFlow Lite
- **CPU latency benchmarking** for performance evaluation
- Deployment-ready **TFLite model**
- Clean and modular project structure

---

## ğŸ› ï¸ Technologies Used
- **Python 3**
- **TensorFlow 2.x**
- **TensorFlow Lite**
- **NumPy**

---

## ğŸ“ Project Structure
memory-efficient-cnn/
â”‚
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ model.py # CNN architecture
â”‚ â”œâ”€â”€ train.py # Model training pipeline
â”‚ â”œâ”€â”€ quantize.py # INT8 TensorFlow Lite quantization
â”‚ â”œâ”€â”€ benchmark.py # CPU inference latency benchmark
â”‚ â””â”€â”€ app.py # Deployment (CPU inference)
â”‚
â”œâ”€â”€ tflite_models/
â”‚ â””â”€â”€ cnn_int8.tflite # Quantized INT8 TFLite model
â”‚
â”œâ”€â”€ reports/
â”‚ â””â”€â”€ benchmark_report.md
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore


---

## ğŸš€ Workflow
1. **Model Design**  
   Built a lightweight CNN using depthwise separable convolution blocks.

2. **Training**  
   Trained the model using TensorFlow on the CIFAR-10 dataset.

3. **Quantization**  
   Applied post-training **full INT8 quantization** using TensorFlow Lite.

4. **Benchmarking**  
   Measured average **CPU inference latency** using the TFLite Interpreter.

5. **Deployment**  
   Deployed the optimized model using a Python-based inference script to simulate edge-device deployment.

---

## ğŸ“Š Results
- ğŸ“¦ **~4Ã— reduction in model size**
- âš¡ **~3Ã— faster CPU inference**
- â±ï¸ **Sub-5 ms average inference latency on CPU**

---
ğŸ’¡ Use Cases

Edge AI applications

Mobile and embedded vision systems

IoT devices with limited compute

Real-time CPU-based inference


## â–¶ï¸ How to Run

### 1ï¸âƒ£ Install dependencies
```bash
pip install -r requirements.txt

2ï¸âƒ£ Train the model
python src/train.py

3ï¸âƒ£ Quantize to INT8
python src/quantize.py

4ï¸âƒ£ Benchmark CPU latency
python src/benchmark.py

5ï¸âƒ£ Run deployment (CPU inference)
python src/app.py
