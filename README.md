ğŸ§  Memory-Efficient CNN for Low-Resource Devices
ğŸ“Œ Overview

This project implements a lightweight Convolutional Neural Network (CNN) optimized for low-resource and edge devices. The model is designed using depthwise separable convolutions to reduce computation and memory usage, and is further optimized using 8-bit (INT8) post-training quantization with TensorFlow Lite.

The optimized model is benchmarked for CPU inference latency and deployed using a Python-based TensorFlow Lite inference application, simulating real-world edge deployment.

ğŸ¯ Key Features

Lightweight CNN architecture using depthwise separable convolutions

Full INT8 quantization using TensorFlow Lite

CPU latency benchmarking for performance evaluation

Deployment-ready TFLite model

Clean and modular project structure

ğŸ› ï¸ Technologies Used

Python 3

TensorFlow 2.x

TensorFlow Lite

NumPy

ğŸ“ Project Structure
memory-efficient-cnn/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ model.py          # CNN architecture
â”‚   â”œâ”€â”€ train.py          # Model training pipeline
â”‚   â”œâ”€â”€ quantize.py       # INT8 TensorFlow Lite quantization
â”‚   â”œâ”€â”€ benchmark.py      # CPU inference latency benchmark
â”‚   â””â”€â”€ app.py            # Deployment (CPU inference)
â”‚
â”œâ”€â”€ tflite_models/
â”‚   â””â”€â”€ cnn_int8.tflite   # Quantized INT8 TFLite model
â”‚
â”œâ”€â”€ reports/
â”‚   â””â”€â”€ benchmark_report.md
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ .gitignore

ğŸš€ Workflow

Model Design
Built a lightweight CNN using depthwise separable convolution blocks.

Training
Trained the model using TensorFlow on the CIFAR-10 dataset.

Quantization
Applied post-training full INT8 quantization using TensorFlow Lite.

Benchmarking
Measured average CPU inference latency using the TFLite Interpreter.

Deployment
Deployed the optimized model using a Python-based inference script to simulate edge-device deployment.

ğŸ“Š Results

ğŸ“¦ ~4Ã— reduction in model size

âš¡ ~3Ã— faster CPU inference

â±ï¸ Sub-5 ms average inference latency on CPU

â–¶ï¸ How to Run
1ï¸âƒ£ Install dependencies
pip install -r requirements.txt

2ï¸âƒ£ Train the model
python src/train.py

3ï¸âƒ£ Quantize to INT8
python src/quantize.py

4ï¸âƒ£ Benchmark CPU latency
python src/benchmark.py

5ï¸âƒ£ Run deployment (CPU inference)
python src/app.py

ğŸ’¡ Use Cases

Edge AI applications

Mobile and embedded vision systems

IoT devices with limited compute

Real-time CPU-based inference

ğŸ§¾ Conclusion

This project demonstrates a complete end-to-end machine learning pipelineâ€”from efficient model design and training to optimization, benchmarking, and deploymentâ€”focused on practical deployment for low-resource environments.

â­ If you like this project

Feel free to â­ the repository and explore further optimizations like pruning or MobileNet-style scaling.
