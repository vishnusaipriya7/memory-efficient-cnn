 Memory-Efficient CNN for Low-Resource Devices
Overview

This project implements a lightweight Convolutional Neural Network (CNN) optimized for low-resource and edge devices. The model is designed using depthwise separable convolutions to reduce computation and memory usage, and is further optimized using 8-bit (INT8) post-training quantization with TensorFlow Lite.

The optimized model is benchmarked for CPU inference latency and deployed using a Python-based TensorFlow Lite inference application, simulating real-world edge deployment.

Key Features

Lightweight CNN architecture using depthwise separable convolutions

Full INT8 quantization using TensorFlow Lite

CPU latency benchmarking for performance evaluation

Deployment-ready TFLite model

Clean and modular project structure

üõ†Ô∏è Technologies Used

Python 3

TensorFlow 2.x

TensorFlow Lite

NumPy

üìÅ Project Structure
memory-efficient-cnn/
‚îÇ
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ model.py          # CNN architecture
‚îÇ   ‚îú‚îÄ‚îÄ train.py          # Model training pipeline
‚îÇ   ‚îú‚îÄ‚îÄ quantize.py       # INT8 TensorFlow Lite quantization
‚îÇ   ‚îú‚îÄ‚îÄ benchmark.py      # CPU inference latency benchmark
‚îÇ   ‚îî‚îÄ‚îÄ app.py            # Deployment (CPU inference)
‚îÇ
‚îú‚îÄ‚îÄ tflite_models/
‚îÇ   ‚îî‚îÄ‚îÄ cnn_int8.tflite   # Quantized INT8 TFLite model
‚îÇ
‚îú‚îÄ‚îÄ reports/
‚îÇ   ‚îî‚îÄ‚îÄ benchmark_report.md
‚îÇ
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ README.md
‚îî‚îÄ‚îÄ .gitignore

Workflow

Model Design
Built a lightweight CNN using depthwise separable convolution blocks.

Training
Trained the model using TensorFlow on the CIFAR-10 dataset.

Quantization
Applied post-training full INT8 quantization using TensorFlow Lite.

Benchmarking
Measured average CPU inference latency using the TFLite Interpreter.

Deployment
Deployed the optimized model using a Python-based inference script to simulate edge-device deployment.

üìä Results

->4√ó reduction in model size

->3√ó faster CPU inference

 Sub-5 ms average inference latency on CPU

‚ñ∂Ô∏è How to Run
1Ô∏è‚É£ Install dependencies
pip install -r requirements.txt

2Ô∏è‚É£ Train the model
python src/train.py

3Ô∏è‚É£ Quantize to INT8
python src/quantize.py

4Ô∏è‚É£ Benchmark CPU latency
python src/benchmark.py

5Ô∏è‚É£ Run deployment (CPU inference)
python src/app.py

üí° Use Cases

Edge AI applications

Mobile and embedded vision systems

IoT devices with limited compute

Real-time CPU-based inference

üßæ Conclusion

This project demonstrates a complete end-to-end machine learning pipeline‚Äîfrom efficient model design and training to optimization, benchmarking, and deployment‚Äîfocused on practical deployment for low-resource environments.

‚≠ê If you like this project

Feel free to ‚≠ê the repository and explore further optimizations like pruning or MobileNet-style scaling.
